{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Ad Performance using Bayesian A/B Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project assesses the performance of different adverts, as measured by Conversion, using several different Bayesian Algorithms applied to A/B testing. The following Algorithms are explored:\n",
    "\n",
    "- Epsilon-Greedy\n",
    "- Optimistic Initial Value\n",
    "- Upper Confidence Bound\n",
    "- Thompson Sampling\n",
    "\n",
    "The dataset used is available here https://www.kaggle.com/osuolaleemmanuel/ad-ab-testing. As well as discussing the intracacies of each algorithm, we also consider their applicability to online learning and marketing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization #############################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read.csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Armed Bandit Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a gambler in a casino who has the option to pull one of several slot machines. The gambler must decide which machine to pull at each iteration in order to maximize some reward. Each machine provides a random reward from some (unknown) probability distribution specific to that machine. The objective is to maximize the total reward from a sequence of lever pulls of the slot machines. An agent simultaneously tries to acquire new knowledge (\"exploration\") whilst at the same time optimizing their decisions based on existing knowledge. The problem requires balancing reward maximization based on the knowledge already acquired with attempting new actions to further increase knowledge. This is known as the exploration-exploitation dilemma in Reinforcement Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific implementation of the Multi-Armed Bandit problem, we consider the Binary multi-armed bandit problem in which each arm, $j$, is a Bernoulli trial that issues a reward of value 1 with probability $p_j$ and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adaptive approach of Bayesian A/B testing is particularly useful for online platforms. Show showing of different adverts or the use of different webpages for conversion. A/B tests can be applied to any problem in which we want to randomly assign different treatments and assess which treatment optimizes some objective function or statistic. For example, this can include validating the positive impact of drugs, selecting from severl webpage or advert designs and the content to show on news feeds based on their engagement. \n",
    "\n",
    "Standard A/B Testing assigns different treatments to a control and (potentially more than one) test group. In medical statistics, this includes sample size calculations for the achievement of a specific test power. The mean value and confidence intervals for the test and control groups are then compared after a pre-determined number of trials, and then the treatment with the best performance is assigned to the whole population. A problem with this approach is that it can often continue to assign the sub-optimal treatment to one of the groups.\n",
    "\n",
    "Also, for sample size calculations, this assumes we have a good estimate of the effect size- if we have strong prior knowledge that we expect one advert to perform better than the other we should be factoring this into our analysis.\n",
    "\n",
    "Another issue with frequentist approaches to hypothesis testing is the subjectivity in statistical significance. The p-value used to choose between treatments can also vary as the number of trials increases, hence a difference in performance between two treatments can vary between significance and insignificance. \n",
    "\n",
    "__Explore-Exploit Dilemma__\n",
    "\n",
    "One one hand, we need enough samples to provide a low-variance estimate of the CTR for each advert but on the other hand we would like to quickly exploit the advert with the highest click-through rate in order to maximize sales and revenue. Bayesian methods provide an effective way of doing this. This is also the main trade-off in reinforcement learning models\n",
    "\n",
    "Bayesian statistics enables one to place distributional assumptions on the estimates of the mean (conversion rate), which gives an improved measure of uncertainty as to the true value of the parameter. We can then use Bayes Theorem to combine the prior and likelihood to get a posterior distribution for the click-through-rate. \n",
    "\n",
    "The discussed algorithms allow us to balance exploration and exploitation. \n",
    "\n",
    "_Exploration_ involves collecting addtional data for each bandit to increase our certainty (reduce the variance) in the estimate of the conversion rate for each bandit. \n",
    "\n",
    "_Exploitation_ involves showing the optimal bandit (the one with the highest MLE of the conversion rate) to consumers, in order to maximize conversion and sales. \n",
    "\n",
    "In particular, we want both a high level of exploration and exploitation, however at each iteration there is a choice between exploration or exploitation- we can't do both at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other problems with the standard frequentist approach to A/B testing are. Standard A/B testing is a purely exploratative technique where we randomly assign users to different versions of an advert or website landing pages usually for some pre-specified number of trials and then we jump straight from this exploration of the choices (bandits) to showing the bandit with the highest win rate to all users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Standard A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- During the exploratory phase information is wasted whilst we continue to explore inferior bandits in order to gather more data\n",
    "\n",
    "- May have to run the test for a long time in order to gather enough data to gain enough statistical confidence to select amongst the bandit\n",
    "\n",
    "- Performance of different bandits may change over time and there may be fluctuations between significance and insignificance of results as more data is gathered. This is particularly prevalent when the assumption of independent trials is violated. \n",
    "\n",
    "- There is a jump from exploration to exploitation, rather than a smooth transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Click Through Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The click through rate is the probability the user clicks an advert/link. Each 'trial' has a Bernoulli distribution.\n",
    "\n",
    "$$CTR = \\frac{No. Clicks}{No. Impressions}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Armed Bandit Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bandit Algorithms attempt to maximize the expected gain of a problem by balancing exploration and exploitation, a key principle of Reinforcement Learning. A fixed set of resources is allocated amongst a set of competing choices whose a priori 'win rates' are unknown. Only by collecting data are we able to estimate the win rate of each choice, but we would also like to quickly exploit the choice that has the highest estimated win rate. Therefore the problem is to gather enough information to accurately estimate the win rate of each bandit, but also to identify and exploit the bandit with the highest win rate as quickly as possible. \n",
    "\n",
    "Instead of two distinct periods of exploration and exploitation (as for standard A/B testing), Multi-Armed Bandits simulataneously carry out exploration and exploitation: the level of exploration is high when little data is gathered but the level of exploitation increases as the amount of data and certainty in estimated win rates increases. Therefore more users will be allocated to better performing bandits as the amount of data increases. \n",
    "\n",
    "Each 'bandit' represents a one of the choices available and at each stage we can choose which bandit we would like to play. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon-Greedy adjusts the 'Greedy' algorithm to randomly allocate one of the bandits with probability $\\epsilon$. In this case, being 'greedy' means choosing the bandit with the highest maximum likelihood estimate. However this can result in being stuck in suboptimal bandits, for example if only one of the bandits returns a sale in the first iteration, it will always have a higher MLE estimate for the conversion rate than the other bandits. Epsilon-Greedy alters this 'greedy' approach by having a small probability of choosing a bandit at random, $\\epsilon$. The controls the amount of exploration- a higher value of $\\epsilon$ is associated with a greater amount of exploration of different bandits but a lower level of exploitation by assigning the optimal bandit (advert) to each user.  We can also introduce a __cooling schedule__, whereby the value of $\\epsilon$ (level of exploration) decreases at each iteration, enabling us to exploit the optimal bandit as we collect more data and hence become more confident in our estimates of net conversion for each bandit. The value of epsilon therefore decays over time, decreasing as the amount of data gathered increases. \n",
    "\n",
    "Epsilon-Greedy is a 'greedy' bandit algorithm. In the context of reinforcement learning, a purely 'greedy' algorithm is one that always plays what it believes to be the best bandit at any given time (i.e. the one with the highest estimated win rate). In other words, it always exploits. However, epsilon-greedy adjusts this approach by randomly selecting another bandit with probability $\\epsilon$, each time a bandit is assigned to a user. This enables the algorithm to explore the space of possible bandits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-code ##################################################################\n",
    "\n",
    "# While TRUE:\n",
    "#   p = random no in [0, 1]\n",
    "#   if p < epsilon:\n",
    "#       j = choose a random bandit\n",
    "#   else:\n",
    "#       j = argmax(estimated bandit means)\n",
    "#   x = play bandit j and get reward bandits[j].\n",
    "#   Update Mean.\n",
    "#   Alter Epsilon using cooling schedule if specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a bandit class which enables us to apply relevant methods to the objects of this class:\n",
    "\n",
    "1) __init__: initialize each probability in the list of bandit probabilities\n",
    "\n",
    "2) __pull__: simulates a True/False outcome when each bandit is played whose value depends on the true mean win rate for the bandit \n",
    "\n",
    "3) __update__: updates the estimated win rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self, p):\n",
    "        # p: win/conversion rate\n",
    "        self.p = p\n",
    "        self.p_estimate = 0\n",
    "        self.N = 0\n",
    "\n",
    "    def pull(self):\n",
    "        # Draw a win (converted) with probability p\n",
    "        # (Generates a Boolean or equivalent binary value for win/ loss)\n",
    "        return np.random.random() < self.p\n",
    "\n",
    "    def update(self, x):\n",
    "        self.N += 1\n",
    "        # update the estimate probability of success\n",
    "        self.p_estimate = (1 / self.N) * ((self.N - 1) * self.p_estimate + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simulate a function that compares the performance of the epsilon-greedy algorithm for different values of the cooling schedule. We consider (identify in the literature which type of cooling schedule is preferred). We then plot graphs to compare the performance of the algorithm with different cooling schedules. Our prior knowledge and intuition suggests that we want a cooling schedule that declines fast enough to exploit the best performing bandit but one that isn't so fast to decline that it fails to explore the bandit and potentially select a sub-optimal bandit. This will require some tuning- we also expect that the more bandits we have and the smaller the gaps in true probabilities, the slower epsilon should decline, since we require more time to explore the bandits in order to identify and then exploit the optimal bandit. Also draw some graphs that illustrate the performance with the number of iterations of each of the different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k greedy\n",
      "Method must be one of:  ['epsilon greedy', 'optimistic initial value', 'upper confidence bound', 'thompson sampling']\n"
     ]
    }
   ],
   "source": [
    "method = \"K greedy\"\n",
    "methods = [\"epsilon greedy\", \"optimistic initial value\", \"upper confidence bound\", \"thompson sampling\"]\n",
    "\n",
    "# Check appropriate arguments for method are used\n",
    "if method.lower() not in methods:\n",
    "    print(method.lower())\n",
    "    print(\"Method must be one of: \", methods)\n",
    "    quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE A PYTHON UNIT TEST FILE IN INTELLIJ IDEA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cooling Rate, EPS, is similar to the cooling schedule in Simulated Anealing and controls the trade-off between exploitation and exploration of the algorithm. Exploring the space of possible solutions will more likely result in the algorithm selecting the Bandit with the greatest conversion, whilst exploiting takes advantage of the improved performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to maximize profit, an online advertiser could use Machine Learning algorithms to improve the conversion of adverts by placing those adverts that yield the highest conversion rate. Moreover, One could split adverts depending on the characterisitics associated with the performance of individual adverts. That is, effctively run different AB tests based on the levels of a variable. However, this results in information loss, if each A/B test is carried out independently. Therefore we require an algorithm (and to perhaps create one if one does not yet exist), that uses information from other A/B tests but also takes account of information in its own AB test to diverge from other groups if required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimistic Initial Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo Code\n",
    "\n",
    "# Initialize bandit means to some large value\n",
    "# While TRUE:\n",
    "#   p = random no in [0, 1]\n",
    "#   if p < epsilon:\n",
    "#       j = choose a random bandit\n",
    "#   else:\n",
    "#       j = argmax(estimated bandit means)\n",
    "#   x = play bandit j and gain reward bandits[j]\n",
    "#   update estimated mean of j\n",
    "#   update epsilon if cooling schedule specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit_OIV:\n",
    "    # Initialize real value of p and estimate of p to large value\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "        self.p_estimate = 10\n",
    "        self.N = 0\n",
    "\n",
    "    # Simulation- generate win with probability p- produces a TRUE/FALSE Boolean\n",
    "    def pull(self):\n",
    "        return(np.random.random() < self.p)\n",
    "\n",
    "    # Update estimated probability of success\n",
    "    def update(self, x):\n",
    "        self.N += 1\n",
    "        self.p_estimate = ((self.N - 1) * self.p_estimate + x) / self.N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo Code ##################################################################\n",
    "\n",
    "# Initialize by playing each bandit once\n",
    "# While TRUE:\n",
    "#   Play j = argmax(Upper Confidence Bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit_UCB:\n",
    "    # Each bandit object should be a probability, p\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "        self.p_estimate = 0\n",
    "        self.N = 0\n",
    "\n",
    "    # Pull function\n",
    "    def pull(self):\n",
    "        return np.random.random() < self.p\n",
    "\n",
    "    # Update estimate of win rate\n",
    "    def update(self, x):\n",
    "        self.N += 1\n",
    "        self.p_estimate = ((self.N - 1) * self.p_estimate + x) / self.N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bayesian A/B testing approach that places a probability distribution on the win rate for each bandit. Prior expectations can also be specified, although typically a non-informative prior is specified. This assumes that all values of the win rates, $p \\in [0, 1]$ are equally likely. Bayes Theorem is then used to calculate the posterior distribution. In order to avoid an intractable integral and a simple calculation, the Beta-Bernoulli conjugate prior can be used. One could also use non-standard priors, however this would require approximation of the integral via Markov Chain Monte Carlo (MCMC) simulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while TRUE:\n",
    "#   sample from posterior for the mean of each bandit j\n",
    "#   play the bandit with the highest simulated mean value\n",
    "#   update the posterior distribution for the selected bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit_TS:\n",
    "    def __init__(self, p):\n",
    "        self.N = 0\n",
    "        self.p = p\n",
    "\n",
    "    # Sample from the posterior distribution for the mean of bandit j\n",
    "    def sample(self):\n",
    "        return np.random.beta(a, b)\n",
    "\n",
    "    def pull(self):\n",
    "\n",
    "\n",
    "    # Update values of a and b in Beta distribution\n",
    "    def update(self):\n",
    "        self.N += 1\n",
    "        a = , b ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our prior domain experience of advertising, we expect click through rates (CTRs) to lie in the region 1% - 5%. Instead of using a Beta[1, 1] prior, which corresponds to the uniform distribution on [0, 1], we can use other parameters [a, b] to represent our prior knowledge of the expected value of the click through rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could simulate some trials, say 100 and then plot the posterior distributions for each bandit to understand and show how the algorithm works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show some graphs displaying how the Thompson sampling technique performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a function to simulate trials. This function works with any of the 4 algorithms we have so far considered, by specifying the required algorithm using the 'method' argument:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we don't have training and test data as such, it is not possible to directly compare each algorithm. Instead, we can assess the performance of each algorithm on simulated data. It has been noted in the literature that the Bayesian Thompson Sampling algorithm is typically the preferred choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simulation(Bandit_Probabilities, cooling_schedule, Num_trials = 10000, EPS = 0.1, method = \"epsilon greedy\"):\n",
    "    \n",
    "    \"\"\"Function to simulate epsilon-greedy bandit algorithm\n",
    "    Inputs: \n",
    "        Bandit_Probabilities: list of known conversion probabilities for each bandit\n",
    "        cooling_schedule: hyperparameter to adjust speed of exploration decay\n",
    "        Num_trials: scalar indicating the number of simulations to perform\n",
    "        EPS: scalar value for epsilon\"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "    possible_methods = [\"epsilon greedy\", \"optimistic initial value\", \"upper confidence bound\", \"thompson sampling\"]\n",
    "\n",
    "    # Check appropriate arguments for method are used\n",
    "    if method.lower() not in methods:\n",
    "        print(\"Method must be one of: \", possible_methods)\n",
    "        quit()\n",
    "        \n",
    "    #Initialize each probability as a Bandit object\n",
    "    bandits = [Bandit(p) for p in Bandit_Probabilities]\n",
    "\n",
    "    #Record metrics\n",
    "    rewards = np.zeros(Num_trials)\n",
    "    num_times_explored = 0\n",
    "    num_times_exploited = 0\n",
    "    num_optimal = 0\n",
    "    optimal_j = np.argmax([b.p for b in bandits])\n",
    "    print(\"optimal j:\", optimal_j)\n",
    "\n",
    "    #Run algorithm\n",
    "    if method == \"epsilon greedy\"\n",
    "    for i in range(Num_trials):\n",
    "\n",
    "        #Use epsilon-greedy to select next bandit\n",
    "        if np.random.random() < EPS:\n",
    "            num_times_explored += 1\n",
    "            #choose random bandit\n",
    "            j = np.random.randint(len(bandits))\n",
    "        else:\n",
    "            num_times_exploited += 1\n",
    "            #choose bandit with optimal p.estimate\n",
    "            j = np.argmax([b.p_estimate for b in bandits])\n",
    "\n",
    "        if j == optimal_j:\n",
    "            num_optimal += 1\n",
    "\n",
    "        #pull arm for bandit with largest sample (generate a 'win' / 'loss')\n",
    "        x = bandits[j].pull()\n",
    "\n",
    "        #update reward log\n",
    "        rewards[i] = x\n",
    "\n",
    "        #Update the distribution for the bandit we selected\n",
    "        bandits[j].update(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal j: 2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bandits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b76fba206c90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mSimulation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBandit_Probabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.35\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcooling_schedule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbandits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mean estimate:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_estimate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bandits' is not defined"
     ]
    }
   ],
   "source": [
    "Num_trials = 10000\n",
    "EPS = 0.1\n",
    "Bandit_Probabilities = [0.3, 0.35, 0.4]\n",
    "\n",
    "Simulation(Bandit_Probabilities = [0.3, 0.35, 0.4], cooling_schedule=0.1)\n",
    "\n",
    "for b in Bandit_Probabilities:\n",
    "    print(\"Mean estimate:\", b.p_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marketing Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have compared and assessed the relative performance of the different algorithms, we will apply Bayesian Thompson Sampling to a marketing application. This algorithm is typically used in this scenario as it optimizes the explore-exploit dilemma relatively well in comparison to the other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which Bandit model was the best?\n",
    "\n",
    "Multi-Armed Bandits provide several advantages over traditional A/B Testing, enabling one to automate the process of bandit selection and optimize opportunity cost and thus sales by simultaneously balancing the explore-exploit dilemma. However, such algorithms require the user to be comfortable handing over decision-making to an automated system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
