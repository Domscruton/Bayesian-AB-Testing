{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Ad Performance using Bayesian A/B Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project assesses the performance of different adverts, as measured by Conversion, using several different Bayesian Algorithms applied to A/B testing. The following Algorithms are explored:\n",
    "\n",
    "- Epsilon-Greedy\n",
    "- Thompson Sampling\n",
    "\n",
    "The dataset used is available here (insert URL). As well as discussing the intracacies of each algorithm, we also consider their applicability to online learning and marketing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with frequentist Hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read.csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-code\n",
    "\n",
    "while TRUE:\n",
    "\n",
    "    p = random no in [0, 1]\n",
    "    if p < epsilon:\n",
    "        j = choose a random bandit\n",
    "    else:\n",
    "        j = argmax(predicted bandit means)\n",
    "    x = play bandit j and get reward bandits[j]. Update Mean. Alter Epsilon using cooling schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_trials = 10000\n",
    "EPS = 0.1\n",
    "Bandit_Probabilities = [0.3, 0.35, 0.4]\n",
    "\n",
    "#Create a Bandit class that initializes each probability in the list of\n",
    "#probabilities and then simulates a True/False outcome (1/0 in Python) when a\n",
    "#particular bandit is played and use the update method to update the\n",
    "#estimated probability\n",
    "class Bandit:\n",
    "    def __init__(self, p):\n",
    "        #p: win/conversion rate\n",
    "        self.p = p\n",
    "        self.p_estimate = 0\n",
    "        self.N = 0\n",
    "\n",
    "    def pull(self):\n",
    "        #Draw a win (converted) with probability p\n",
    "        return(np.random.random() < self.p)\n",
    "\n",
    "    def update(self, x):\n",
    "        self.N += 1\n",
    "        #update the estimate probability of success\n",
    "        self.p_estimate = (1 / self.N) * ((self.N - 1) * self.p_estimate + x)\n",
    "\n",
    "def Simulation(Num_trials, EPS, Bandit_Probabilities, cooling_schedule):\n",
    "    #Initialize each probability as a Bandit object\n",
    "    bandits = [Bandit(p) for p in Bandit_Probabilities]\n",
    "\n",
    "    #Record metrics\n",
    "    rewards = np.zeros(Num_trials)\n",
    "    num_times_explored = 0\n",
    "    num_times_exploited = 0\n",
    "    num_optimal = 0\n",
    "    optimal_j = np.argmax([b.p for b in bandits])\n",
    "    print(\"optimal j:\", optimal_j)\n",
    "\n",
    "    #Run algorithm\n",
    "    for i in range(Num_trials):\n",
    "\n",
    "        #Use epsilon-greedy to select next bandit\n",
    "        if np.random.random() < EPS:\n",
    "            num_times_explored += 1\n",
    "            #choose random bandit\n",
    "            j = np.random.randint(len(bandits))\n",
    "        else:\n",
    "            num_times_exploited += 1\n",
    "            #choose bandit with optimal p.estimate\n",
    "            j = np.argmax([b.p_estimate for b in bandits])\n",
    "\n",
    "        if j == optimal_j:\n",
    "            num_optimal += 1\n",
    "\n",
    "        #pull arm for bandit with largest sample (generate a 'win' / 'loss')\n",
    "        x = bandits[j].pull()\n",
    "\n",
    "        #update reward log\n",
    "        rewards[i] = x\n",
    "\n",
    "        #Update the distribution for the bandit we selected\n",
    "        bandits[j].update(x)\n",
    "\n",
    "Simulation()\n",
    "\n",
    "for b in bandits:\n",
    "    print(\"Mean estimate:\", b.p_estimate)\n",
    "\n",
    "# Declining Epsilon (Cooling Schedule) #########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cooling Rate, EPS, is similar to the cooling schedule in Simulated Anealing and controls the trade-off between exploitation and exploration of the algorithm. Exploring the space of possible solutions will more likely result in the algorithm selecting the Bandit with the greatest conversion, whilst exploiting takes advantage of the improved performance. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
