{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Ad Performance using Bayesian A/B Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project assesses the performance of different adverts, as measured by Conversion, using several different Bayesian Algorithms applied to A/B testing. The following Algorithms are explored:\n",
    "\n",
    "- Epsilon-Greedy\n",
    "- Optimistic Initial Value\n",
    "- Upper Confidence Bound\n",
    "- Thompson Sampling\n",
    "\n",
    "The dataset used is available here https://www.kaggle.com/osuolaleemmanuel/ad-ab-testing. As well as discussing the intracacies of each algorithm, we also consider their applicability to online learning and marketing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with frequentist Hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read.csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adaptive approach of Bayesian A/B testing is particularly useful for online platforms. Show showing of different adverts or the use of different webpages for conversion. A/B tests can be applied to any problem in which we want to randomly assign different treatments and assess which treatment optimizes some objective function or statistic. For example, this can include validating the positive impact of drugs, selecting from severl webpage or advert designs and the content to show on news feeds based on their engagement. \n",
    "\n",
    "Standard A/B Testing assigns different treatments to a control and (potentially more than one) test group. In medical statistics, this includes sample size calculations for the achievement of a specific test power. The mean value and confidence intervals for the test and control groups are then compared after a pre-determined number of trials, and then the treatment with the best performance is assigned to the whole population. A problem with this approach is that it can often continue to assign the sub-optimal treatment to one of the groups.\n",
    "\n",
    "Also, for sample size calculations, this assumes we have a good estimate of the effect size- if we have strong prior knowledge that we expect one advert to perform better than the other we should be factoring this into our analysis.\n",
    "\n",
    "Another issue with frequentist approaches to hypothesis testing is the subjectivity in statistical significance. The p-value used to choose between treatments can also vary as the number of trials increases, hence a difference in performance between two treatments can vary between significance and insignificance. \n",
    "\n",
    "__Explore-Exploit Dilemma__\n",
    "\n",
    "One one hand, we need enough samples to provide a low-variance estimate of the CTR for each advert but on the other hand we would like to quickly exploit the advert with the highest click-through rate in order to maximize sales and revenue. Bayesian methods provide an effective way of doing this. This is also the main trade-off in reinforcement learning models\n",
    "\n",
    "Bayesian statistics enables one to place distributional assumptions on the estimates of the mean (conversion rate), which gives an improved measure of uncertainty as to the true value of the parameter. We can then use Bayes Theorem to combine the prior and likelihood to get a posterior distribution for the click-through-rate. \n",
    "\n",
    "The discussed algorithms allow us to balance exploration and exploitation. \n",
    "\n",
    "_Exploration_ involves collecting addtional data for each bandit to increase our certainty (reduce the variance) in the estimate of the conversion rate for each bandit. \n",
    "\n",
    "_Exploitation_ involves showing the optimal bandit (the one with the highest MLE of the conversion rate) to consumers, in order to maximize conversion and sales. \n",
    "\n",
    "In particular, we want both a high level of exploration and exploitation, however at each iteration there is a choice between exploration or exploitation- we can't do both at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other problems with the standard frequentist approach to A/B testing are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Standard A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- During the exploratory phase information is wasted whilst we continue to explore inferior bandits in order to gather more data\n",
    "\n",
    "- May have to run the test for a long time in order to gather enough data to gain enough statistical confidence to select amongst the bandit\n",
    "\n",
    "- Performance of different bandits may change over time and there may be fluctuations between significance and insignificance of results as more data is gathered. This is particularly prevalent when the assumption of independent trials is violated. \n",
    "\n",
    "- There is a jump from exploration to exploitation, rather than a smooth transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Click Through Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The click through rate is the probability the user clicks an advert/link. Each 'trial' has a Bernoulli distribution.\n",
    "\n",
    "$$CTR = \\frac{No. Clicks}{No. Impressions}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon-Greedy adjusts the 'Greedy' algorithm to randomly allocate one of the bandits with probability $\\epsilon$. In this case, being 'greedy' means choosing the bandit with the highest maximum likelihood estimate. However this can result in being stuck in suboptimal bandits, for example if only one of the bandits returns a sale in the first iteration, it will always have a higher MLE estimate for the conversion rate than the other bandits. Epsilon-Greedy alters this 'greedy' approach by having a small probability of choosing a bandit at random, $\\epsilon$. The controls the amount of exploration- a higher value of $\\epsilon$ is associated with a greater amount of exploration of different bandits but a lower level of exploitation by assigning the optimal bandit (advert) to each user.  We can also introduce a __cooling schedule__, whereby the value of $\\epsilon$ (level of exploration) decreases at each iteration, enabling us to exploit the optimal bandit as we collect more data and hence become more confident in our estimates of net conversion for each bandit. The value of epsilon therefore decays over time, decreasing as the amount of data gathered increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-code\n",
    "\n",
    "while TRUE:\n",
    "\n",
    "    p = random no in [0, 1]\n",
    "    if p < epsilon:\n",
    "        j = choose a random bandit\n",
    "    else:\n",
    "        j = argmax(predicted bandit means)\n",
    "    x = play bandit j and get reward bandits[j]. Update Mean. Alter Epsilon using cooling schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-4-abf9d29137eb>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-abf9d29137eb>\"\u001b[1;36m, line \u001b[1;32m25\u001b[0m\n\u001b[1;33m    def Simulation(Num_trials = 10000, EPS = 0.1, Bandit_Probabilities, cooling_schedule):\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "Num_trials = 10000\n",
    "EPS = 0.1\n",
    "Bandit_Probabilities = [0.3, 0.35, 0.4]\n",
    "\n",
    "#Create a Bandit class that initializes each probability in the list of\n",
    "#probabilities and then simulates a True/False outcome (1/0 in Python) when a\n",
    "#particular bandit is played and use the update method to update the\n",
    "#estimated probability\n",
    "class Bandit:\n",
    "    def __init__(self, p):\n",
    "        #p: win/conversion rate\n",
    "        self.p = p\n",
    "        self.p_estimate = 0\n",
    "        self.N = 0\n",
    "\n",
    "    def pull(self):\n",
    "        #Draw a win (converted) with probability p\n",
    "        return(np.random.random() < self.p)\n",
    "\n",
    "    def update(self, x):\n",
    "        self.N += 1\n",
    "        #update the estimate probability of success\n",
    "        self.p_estimate = (1 / self.N) * ((self.N - 1) * self.p_estimate + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a function to simulate trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simulation(Bandit_Probabilities, cooling_schedule, Num_trials = 10000, EPS = 0.1):\n",
    "    \n",
    "    \"\"\"Function to simulate epsilon-greedy bandit algorithm\n",
    "    Inputs: \n",
    "        Bandit_Probabilities: list of known conversion probabilities for each bandit\n",
    "        cooling_schedule: hyperparameter to adjust speed of exploration decay\n",
    "        Num_trials: scalar indicating the number of simulations to perform\n",
    "        EPS: scalar value for epsilon\"\"\"\n",
    "    \n",
    "    pass\n",
    "    \n",
    "    # Inputs:\n",
    "        # \n",
    "    \n",
    "    #Initialize each probability as a Bandit object\n",
    "    bandits = [Bandit(p) for p in Bandit_Probabilities]\n",
    "\n",
    "    #Record metrics\n",
    "    rewards = np.zeros(Num_trials)\n",
    "    num_times_explored = 0\n",
    "    num_times_exploited = 0\n",
    "    num_optimal = 0\n",
    "    optimal_j = np.argmax([b.p for b in bandits])\n",
    "    print(\"optimal j:\", optimal_j)\n",
    "\n",
    "    #Run algorithm\n",
    "    for i in range(Num_trials):\n",
    "\n",
    "        #Use epsilon-greedy to select next bandit\n",
    "        if np.random.random() < EPS:\n",
    "            num_times_explored += 1\n",
    "            #choose random bandit\n",
    "            j = np.random.randint(len(bandits))\n",
    "        else:\n",
    "            num_times_exploited += 1\n",
    "            #choose bandit with optimal p.estimate\n",
    "            j = np.argmax([b.p_estimate for b in bandits])\n",
    "\n",
    "        if j == optimal_j:\n",
    "            num_optimal += 1\n",
    "\n",
    "        #pull arm for bandit with largest sample (generate a 'win' / 'loss')\n",
    "        x = bandits[j].pull()\n",
    "\n",
    "        #update reward log\n",
    "        rewards[i] = x\n",
    "\n",
    "        #Update the distribution for the bandit we selected\n",
    "        bandits[j].update(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Simulation() missing 2 required positional arguments: 'Num_trials' and 'EPS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b76fba206c90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSimulation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBandit_Probabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.35\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcooling_schedule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbandits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mean estimate:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_estimate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Simulation() missing 2 required positional arguments: 'Num_trials' and 'EPS'"
     ]
    }
   ],
   "source": [
    "Simulation(Bandit_Probabilities = [0.3, 0.35, 0.4], cooling_schedule=0.1)\n",
    "\n",
    "for b in bandits:\n",
    "    print(\"Mean estimate:\", b.p_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cooling Rate, EPS, is similar to the cooling schedule in Simulated Anealing and controls the trade-off between exploitation and exploration of the algorithm. Exploring the space of possible solutions will more likely result in the algorithm selecting the Bandit with the greatest conversion, whilst exploiting takes advantage of the improved performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to maximize profit, an online advertiser could use Machine Learning algorithms to improve the conversion of adverts by placing those adverts that yield the highest conversion rate. Moreover, One could split adverts depending on the characterisitics associated with the performance of individual adverts. That is, effctively run different AB tests based on the levels of a variable. However, this results in information loss, if each A/B test is carried out independently. Therefore we require an algorithm (and to perhaps create one if one does not yet exist), that uses information from other A/B tests but also takes account of information in its own AB test to diverge from other groups if required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our prior domain experience of advertising, we expect click through rates (CTRs) to lie in the region 1% - 5%. Instead of using a Beta[1, 1] prior, which corresponds to the uniform distribution on [0, 1], we can use other parameters [a, b] to represent our prior knowledge of the expected value of the click through rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could simulate some trials, say 100 and then plot the posterior distributions for each bandit to understand and show how the algorithm works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marketing Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
